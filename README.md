# TP2 â€” Pipeline d'acquisition et transformation de donnÃ©es

Pipeline automatisÃ© pour rÃ©cupÃ©rer des donnÃ©es Open Data via API, les nettoyer et les stocker dans un format optimisÃ© pour l'analyse.

## ğŸ¯ Objectifs

- âœ… Interroger une API REST Open Data (OpenFoodFacts)
- âœ… GÃ©rer la pagination et les erreurs
- âœ… Transformer et nettoyer des donnÃ©es avec l'aide de l'IA
- âœ… Stocker des donnÃ©es au format Parquet
- âœ… Construire un pipeline reproductible

## ğŸ“‹ PrÃ©requis

- Python 3.10+
- [uv](https://github.com/astral-sh/uv) (gestionnaire de paquets moderne)

## ğŸš€ Installation

```bash
# Installer les dÃ©pendances
uv sync

# OU avec pip
pip install -r requirements.txt
```

## ğŸ“ Structure du projet

```
tp2-pipeline/
â”œâ”€â”€ pipeline/              # Modules du pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”œâ”€â”€ fetcher.py         # RÃ©cupÃ©ration des donnÃ©es
â”‚   â”œâ”€â”€ transformer.py     # Nettoyage et transformation
â”‚   â”œâ”€â”€ storage.py         # Stockage Parquet
â”‚   â””â”€â”€ main.py            # Orchestration
â”œâ”€â”€ tests/                 # Tests unitaires
â”œâ”€â”€ notebooks/             # Notebooks d'exploration
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ data/                  # DonnÃ©es (crÃ©Ã© automatiquement)
â”‚   â”œâ”€â”€ raw/               # DonnÃ©es brutes JSON
â”‚   â””â”€â”€ processed/         # DonnÃ©es nettoyÃ©es Parquet
â””â”€â”€ logs/                  # Logs (crÃ©Ã© automatiquement)
```

## ğŸ”§ Utilisation

### ExÃ©cution du pipeline

```bash
# CatÃ©gorie par dÃ©faut (chocolats)
uv run python -m pipeline.main

# Changer de catÃ©gorie
uv run python -m pipeline.main --category biscuits --name biscuits_fr
uv run python -m pipeline.main --category boissons --name boissons_fr
uv run python -m pipeline.main --category yaourts --name yaourts_fr

# Avec suggestions IA (nÃ©cessite clÃ© API dans .env)
uv run python -m pipeline.main --category chocolats --name chocolats_fr --ai-cleaning

# Mode verbose
uv run python -m pipeline.main --category chocolats --name chocolats_fr --verbose
```

### CatÃ©gories disponibles

- `chocolats`, `biscuits`, `boissons`, `yaourts`, `pates`, `pizzas`, `fromages`, `pain`, `cereales`, `fruits`, `legumes`, `viandes`, `poissons`

### VÃ©rification des donnÃ©es

```bash
uv run python verify_data.py
```

### Exploration avec Jupyter

```bash
# Lancer Jupyter
uv run jupyter notebook

# OU JupyterLab
uv run jupyter lab
```

Puis ouvrir `notebooks/exploration.ipynb` et modifier `CATEGORY_NAME` pour changer de catÃ©gorie.

### Exploration de l'API

```bash
uv run python exploration_api.py
```

## ğŸ§ª Tests

```bash
# Tous les tests
uv run pytest tests/ -v

# Avec couverture
uv run pytest tests/ --cov=pipeline --cov-report=html
```

## ğŸ³ Docker

```bash
# Construction
docker build -t tp2-pipeline .

# ExÃ©cution
docker run --rm -v $(pwd)/data:/app/data tp2-pipeline --category chocolats --name chocolats_fr

# Avec Docker Compose
docker-compose run pipeline --category chocolats --name chocolats_fr
```

## ğŸ“Š FonctionnalitÃ©s

### Acquisition des donnÃ©es
- RÃ©cupÃ©ration paginÃ©e avec retry automatique
- Gestion des erreurs et timeouts
- Respect du rate limiting
- Logging dÃ©taillÃ©

### Transformation
- Conversion en DataFrame pandas
- GÃ©nÃ©ration de code de nettoyage avec IA (optionnel)
- Nettoyage automatique :
  - Suppression des doublons
  - Gestion des valeurs manquantes
  - Normalisation des textes
  - Correction des valeurs aberrantes
  - Gestion des outliers

### Stockage
- Sauvegarde JSON pour les donnÃ©es brutes
- Stockage Parquet optimisÃ© (compression snappy)
- Timestamping automatique

## ğŸ” API utilisÃ©e : OpenFoodFacts

- **Base URL** : `https://world.openfoodfacts.org/api/v2`
- **Documentation** : https://openfoodfacts.github.io/openfoodfacts-server/api/
- **Pas d'authentification requise**
- **Rate limit** : 1 requÃªte/seconde (respectÃ© automatiquement)

## ğŸ Bonus implÃ©mentÃ©s

### âœ… Tests unitaires (+2 points)
- Tests complets pour tous les modules
- Couverture de code avec pytest-cov

### âœ… Logging (+1 point)
- Module logging configurÃ©
- Logs structurÃ©s dans `logs/pipeline.log`

### âœ… Dockerisation (+1 point)
- Dockerfile optimisÃ© multi-stage
- Docker Compose pour faciliter l'utilisation

## ğŸ“ Configuration IA (optionnel)

Le pipeline fonctionne parfaitement sans IA. Pour activer les suggestions IA :

1. CrÃ©er un fichier `.env` :
```env
GEMINI_API_KEY=votre_cle_api
```

2. Obtenir une clÃ© gratuite : https://aistudio.google.com/app/apikey

3. Utiliser le flag `--ai-cleaning` lors de l'exÃ©cution

## ğŸ› DÃ©pannage

### Erreur de connexion API
- VÃ©rifier votre connexion internet
- Augmenter `API_TIMEOUT` dans `pipeline/config.py`

### Pas de donnÃ©es rÃ©cupÃ©rÃ©es
- VÃ©rifier que la catÃ©gorie existe sur OpenFoodFacts
- Consulter les logs dans `logs/pipeline.log`

### Erreur Parquet
- VÃ©rifier que `pyarrow` est installÃ©
- VÃ©rifier les permissions d'Ã©criture dans `data/processed/`

## ğŸ“„ Licence

Ce projet est rÃ©alisÃ© dans le cadre d'un TP pÃ©dagogique.
