"""Script principal du pipeline."""
import argparse
import logging
import sys

from .fetcher import fetch_all_data
from .transformer import raw_to_dataframe, clean_dataframe
from .storage import save_raw_json, save_parquet
from .config import logger, AVAILABLE_CATEGORIES

logger = logging.getLogger(__name__)


def run_pipeline(category: str, name: str, use_ai_cleaning: bool = False) -> str:
    """
    Ex√©cute le pipeline complet.
    
    Args:
        category: Cat√©gorie √† r√©cup√©rer
        name: Nom pour les fichiers de sortie
        use_ai_cleaning: Si True, utilise l'IA pour g√©n√©rer des suggestions de nettoyage
    
    Returns:
        Chemin du fichier Parquet cr√©√©
    """
    logger.info("=" * 50)
    logger.info(f"PIPELINE : {name}")
    logger.info("=" * 50)
    
    try:
        # √âtape 1 : Acquisition
        logger.info("\nüì• √âtape 1 : Acquisition des donn√©es")
        raw_data = fetch_all_data(category)
        raw_file = save_raw_json(raw_data, name)
        logger.info(f"‚úÖ Donn√©es brutes sauvegard√©es : {raw_file}")
        
        # √âtape 2 : Transformation
        logger.info("\nüîß √âtape 2 : Transformation")
        df = raw_to_dataframe(raw_data)
        df_clean = clean_dataframe(df, use_ai_suggestions=use_ai_cleaning)
        logger.info(f"‚úÖ Donn√©es transform√©es : {df_clean.shape}")
        
        # √âtape 3 : Stockage
        logger.info("\nüíæ √âtape 3 : Stockage")
        output_path = save_parquet(df_clean, name)
        logger.info(f"‚úÖ Donn√©es sauvegard√©es : {output_path}")
        
        logger.info("\n" + "=" * 50)
        logger.info("‚úÖ Pipeline termin√© avec succ√®s !")
        logger.info(f"üìÅ Fichier : {output_path}")
        logger.info("=" * 50)
        
        return output_path
        
    except ValueError as e:
        logger.error(f"‚ùå Erreur de validation : {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ex√©cution du pipeline : {e}", exc_info=True)
        sys.exit(1)


def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description="Pipeline Open Data - Acquisition et transformation de donn√©es",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  python -m pipeline.main --category chocolats --name chocolats_fr
  python -m pipeline.main --category biscuits --name biscuits_fr --ai-cleaning
        """
    )
    parser.add_argument(
        "--category",
        default="chocolats",
        help=f"Cat√©gorie √† r√©cup√©rer (d√©faut: chocolats). Cat√©gories disponibles: {', '.join(AVAILABLE_CATEGORIES[:5])}..."
    )
    parser.add_argument(
        "--name",
        default="products",
        help="Nom du dataset (d√©faut: products)"
    )
    parser.add_argument(
        "--ai-cleaning",
        action="store_true",
        help="Utiliser l'IA pour g√©n√©rer des suggestions de nettoyage"
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Mode verbose (niveau DEBUG)"
    )
    
    args = parser.parse_args()
    
    # Ajuster le niveau de log
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.setLevel(logging.DEBUG)
    
    run_pipeline(args.category, args.name, args.ai_cleaning)


if __name__ == "__main__":
    main()

